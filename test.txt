# rag_system.py
# Handles the core RAG system logic for the Streamlit RAG app

import os
import glob
import logging
from datetime import datetime
from typing import List, Dict, Any
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.memory import ConversationBufferWindowMemory
from langchain_groq import ChatGroq
from evaluation import EvaluationDataset, RAGEvaluator
import statistics

from config import (
    GROQ_API_KEY, GROQ_MODEL_NAME, EMBEDDING_MODEL_NAME, DOCUMENTS_FOLDER,
    CHUNK_SIZE, CHUNK_OVERLAP, COLLECTION_NAME
)

logger = logging.getLogger(__name__)

class LangChainRAGSystem:
    """RAG System using LangChain with Groq and ChromaDB"""
    def __init__(self):
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.qa_chain = None
        self.memory = None
        self.text_splitter = None
        self.evaluator = None
        self.evaluation_dataset = EvaluationDataset()
    def initialize_components(self, groq_api_key: str = None):
        try:
            api_key = groq_api_key or GROQ_API_KEY
            if not api_key:
                raise ValueError("Groq API key not provided in code or parameter")
            self.embeddings = HuggingFaceEmbeddings(
                model_name=EMBEDDING_MODEL_NAME,
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            self.llm = ChatGroq(
                groq_api_key=api_key,
                model_name=GROQ_MODEL_NAME,
                temperature=0.1,
                max_tokens=1000
            )
            # Sliding window chunking
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=CHUNK_SIZE,
                chunk_overlap=CHUNK_OVERLAP,
                length_function=len,
                separators=["\n\n", "\n", " ", ""]
            )
            self.memory = ConversationBufferWindowMemory(
                memory_key="chat_history",
                return_messages=True,
                k=6
            )
            logger.info("All components initialized successfully")
            return True
        except Exception as e:
            logger.error(f"Error initializing components: {e}")
            return False
    def load_documents(self, folder_path: str) -> List[Document]:
        documents = []
        if not os.path.exists(folder_path):
            logger.error(f"Documents folder not found: {folder_path}")
            return documents
        txt_files = glob.glob(os.path.join(folder_path, "*.txt"))
        for file_path in txt_files:
            try:
                loader = TextLoader(file_path, encoding='utf-8')
                docs = loader.load()
                for doc in docs:
                    doc.metadata.update({
                        'source_file': os.path.basename(file_path),
                        'source_path': file_path
                    })
                    documents.extend([doc])
                logger.info(f"Loaded document: {os.path.basename(file_path)}")
            except Exception as e:
                logger.error(f"Error loading {file_path}: {e}")
        return documents
    def create_vectorstore(self, folder_path: str):
        try:
            documents = self.load_documents(folder_path)
            if not documents:
                logger.warning("No documents found to process")
                return False
            texts = self.text_splitter.split_documents(documents)
            self.vectorstore = Chroma.from_documents(
                documents=texts,
                embedding=self.embeddings,
                collection_name=COLLECTION_NAME,
                persist_directory="./chroma_db"
            )
            self.vectorstore.persist()
            logger.info(f"Created vectorstore with {len(texts)} chunks")
            return True
        except Exception as e:
            logger.error(f"Error creating vectorstore: {e}")
            return False
    def generate_step_back_question(self, original_question: str) -> str:
        step_back_prompt = f"""Given the following specific question about GitHub API:\n"{original_question}"\n\nGenerate a broader, more general question that would help understand the fundamental concepts needed to answer the original question. The step-back question should focus on general principles or broader categories.\n\nExamples:\n- Specific: \"How do I create a repository using POST /user/repos?\"\n- Step-back: \"What are the general patterns for creating resources via GitHub API?\"\n\n- Specific: \"How do I authenticate with personal access tokens?\"\n- Step-back: \"What are the different authentication methods available in GitHub API?\"\n\nStep-back question:"""
        try:
            response = self.llm.invoke(step_back_prompt)
            return response.content.strip()
        except Exception as e:
            logger.error(f"Error generating step-back question: {e}")
            return f"What are the general principles and concepts related to {original_question}?"
    def setup_qa_chain(self):
        try:
            prompt_template = """You are Zoro, an expert AI assistant created by Balaji, specializing in GitHub API documentation.\n\nSTEP-BACK REASONING APPROACH:\n1. First, I'll consider the broader context and fundamental principles\n2. Then, I'll provide specific, actionable guidance for the user's question\n\nStep-back question: {step_back_question}\nBroader context from documentation: {step_back_context}\n\nOriginal question: {question}\nSpecific context from documentation: {context}\n\nChat History: {chat_history}\n\nREASONING PROCESS:\nLet me think through this systematically:\n\n1. **Understanding the broader context**: Based on the step-back analysis, what are the fundamental principles at play?\n\n2. **Connecting to specific needs**: How do these general principles apply to the user's specific question?\n\n3. **Practical implementation**: What are the concrete steps the user should take?\n\nRESPONSE GUIDELINES:\n- Start with essential concepts if helpful for understanding\n- Provide practical, actionable guidance\n- Include relevant code examples or API endpoint details\n- Keep responses comprehensive but concise\n- Use the retrieved documentation context as your primary source\n\nAnswer:"""
            prompt = PromptTemplate(
                template=prompt_template,
                input_variables=["context", "question", "chat_history", "step_back_question", "step_back_context"]
            )
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 3}
                ),
                return_source_documents=True
            )
            logger.info("QA chain setup successfully with step-back prompting")
            return True
        except Exception as e:
            logger.error(f"Error setting up QA chain: {e}")
            return False
    def get_response(self, question: str) -> Dict[str, Any]:
        try:
            step_back_question = self.generate_step_back_question(question)
            original_docs = self.vectorstore.similarity_search(question, k=3)
            step_back_docs = self.vectorstore.similarity_search(step_back_question, k=2)
            original_context = "\n\n".join([doc.page_content for doc in original_docs])
            step_back_context = "\n\n".join([doc.page_content for doc in step_back_docs])
            chat_history = self.memory.chat_memory.messages if self.memory else []
            history_text = "\n".join([f"{msg.type}: {msg.content}" for msg in chat_history[-6:]])
            enhanced_prompt = f"""You are Zoro, an expert AI assistant created by Balaji, specializing in GitHub API documentation.\n\nSTEP-BACK REASONING APPROACH:\n1. First, I'll consider the broader context and fundamental principles\n2. Then, I'll provide specific, actionable guidance for the user's question\n\nStep-back question: {step_back_question}\nBroader context from documentation: {step_back_context}\n\nOriginal question: {question}\nSpecific context from documentation: {original_context}\n\nChat History: {history_text}\n\nREASONING PROCESS:\nLet me think through this systematically:\n\n1. **Understanding the broader context**: Based on the step-back analysis, what are the fundamental principles at play?\n\n2. **Connecting to specific needs**: How do these general principles apply to the user's specific question?\n\n3. **Practical implementation**: What are the concrete steps the user should take?\n\nRESPONSE GUIDELINES:\n- Start with essential concepts if helpful for understanding\n- Provide practical, actionable guidance\n- Include relevant code examples or API endpoint details\n- Keep responses comprehensive but concise\n- Use the retrieved documentation context as your primary source\n\nAnswer:"""
            response = self.llm.invoke(enhanced_prompt)
            all_docs = original_docs + step_back_docs
            sources = list(set([
                doc.metadata.get('source_file', 'Unknown')
                for doc in all_docs
            ]))
            self.memory.chat_memory.add_user_message(question)
            self.memory.chat_memory.add_ai_message(response.content)
            return {
                "response": response.content,
                "sources": sources,
                "source_documents": len(all_docs),
                "step_back_question": step_back_question,
                "retrieved_chunks": [
                    {
                        "content": doc.page_content[:300] + "..." if len(doc.page_content) > 300 else doc.page_content,
                        "source": doc.metadata.get('source_file', 'Unknown'),
                        "type": "original" if doc in original_docs else "step_back"
                    }
                    for doc in all_docs
                ],
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return {
                "response": "I apologize, but I encountered an error while processing your request. Please try again.",
                "sources": [],
                "source_documents": 0,
                "error": str(e)
            }
    def clear_memory(self):
        if self.memory:
            self.memory.clear()
    def initialize_evaluator(self):
        self.evaluator = RAGEvaluator(self)
        logger.info("Evaluation system initialized")
    def run_evaluation(self) -> Dict[str, Any]:
        if not self.evaluator:
            self.initialize_evaluator()
        qa_pairs = self.evaluation_dataset.get_qa_pairs()
        results = []
        import streamlit as st
        st.write("🔄 Running evaluation...")
        progress_bar = st.progress(0)
        for i, qa_pair in enumerate(qa_pairs):
            question = qa_pair["question"]
            try:
                rag_response = self.get_response(question)
                predicted_response = rag_response["response"]
                evaluation_result = self.evaluator.evaluate_response(
                    question, predicted_response, qa_pair
                )
                evaluation_result["rag_metadata"] = {
                    "sources": rag_response.get("sources", []),
                    "source_documents": rag_response.get("source_documents", 0),
                    "step_back_question": rag_response.get("step_back_question", "")
                }
                results.append(evaluation_result)
                progress_bar.progress((i + 1) / len(qa_pairs))
            except Exception as e:
                logger.error(f"Error evaluating question '{question}': {e}")
                results.append({
                    "question": question,
                    "error": str(e),
                    "timestamp": datetime.now().isoformat()
                })
        valid_results = [r for r in results if "error" not in r]
        if valid_results:
            f1_scores = [r["f1_metrics"]["f1"] for r in valid_results]
            rouge1_scores = [r["rouge_scores"]["rouge1_f"] for r in valid_results]
            keyword_coverages = [r["keyword_coverage"] for r in valid_results]
            aggregate_metrics = {
                "f1_score": {
                    "mean": statistics.mean(f1_scores),
                    "std": statistics.stdev(f1_scores) if len(f1_scores) > 1 else 0.0,
                    "min": min(f1_scores),
                    "max": max(f1_scores)
                },
                "rouge1_f": {
                    "mean": statistics.mean(rouge1_scores),
                    "std": statistics.stdev(rouge1_scores) if len(rouge1_scores) > 1 else 0.0
                },
                "keyword_coverage": {
                    "mean": statistics.mean(keyword_coverages),
                    "std": statistics.stdev(keyword_coverages) if len(keyword_coverages) > 1 else 0.0
                }
            }
        else:
            aggregate_metrics = {"error": "No valid results to aggregate"}
        return {
            "total_questions": len(qa_pairs),
            "successful_evaluations": len(valid_results),
            "failed_evaluations": len([r for r in results if "error" in r]),
            "aggregate_metrics": aggregate_metrics,
            "individual_results": results,
            "evaluation_timestamp": datetime.now().isoformat()
        } 